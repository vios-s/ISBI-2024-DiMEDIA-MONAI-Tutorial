{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51618a18-7bda-4d87-bce4-06ecfe9a0889",
   "metadata": {},
   "source": [
    "# ISBI 2024 DiMEDIA: MONAI Tutorial: DDIM + Classifier Free Guidance\n",
    "\n",
    "This tutorial illustrates how to use MONAI for training a denoising diffusion implicit model (DDIM) to create synthetic 2D images using the classifier-free guidance technique [1] to perform conditioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "226301c8-105a-43fd-8179-38692f5ca53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.0\n",
      "Numpy version: 1.26.2\n",
      "Pytorch version: 2.2.2+cu118\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
      "MONAI __file__: /remote/rds/users/<username>/miniforge3/envs/dev/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.3.0\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.22.0\n",
      "scipy version: 1.11.4\n",
      "Pillow version: 10.3.0\n",
      "Tensorboard version: 2.16.2\n",
      "gdown version: 4.7.3\n",
      "TorchVision version: 0.17.2+cu118\n",
      "tqdm version: 4.66.1\n",
      "lmdb version: 1.4.1\n",
      "psutil version: 5.9.5\n",
      "pandas version: 2.1.4\n",
      "einops version: 0.7.0\n",
      "transformers version: 4.39.3\n",
      "mlflow version: 2.11.3\n",
      "pynrrd version: 1.0.0\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.utils import first, set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "print_config()\n",
    "set_determinism(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707db2fb-aa0c-4681-9d49-2768fafb697e",
   "metadata": {},
   "source": [
    "## Setup MedNIST Dataset\n",
    "\n",
    "In this tutorial, we will train our models on the MedNIST dataset available on MONAI `https://docs.monai.io/en/stable/apps.html#monai.apps.MedNISTDataset`. \n",
    "\n",
    "Here, we will use the `Hand` and `HeadCT`, where our conditioning variable `class` will specify the modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15986f04-84e5-4b5e-8d4f-4a10047cca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-24 08:59:34,121 - INFO - Verified 'MedNIST.tar.gz', md5: 0bc7306e7427e00ad1c5526a6677552d.\n",
      "2024-05-24 08:59:34,122 - INFO - File exists: data/MedNIST.tar.gz, skipped downloading.\n",
      "2024-05-24 08:59:34,125 - INFO - Non-empty folder exists in data/MedNIST, skipped extracting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████| 15990/15990 [00:50<00:00, 318.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-24 09:02:20,269 - INFO - Verified 'MedNIST.tar.gz', md5: 0bc7306e7427e00ad1c5526a6677552d.\n",
      "2024-05-24 09:02:20,270 - INFO - File exists: data/MedNIST.tar.gz, skipped downloading.\n",
      "2024-05-24 09:02:20,272 - INFO - Non-empty folder exists in data/MedNIST, skipped extracting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████| 1977/1977 [00:04<00:00, 454.55it/s]\n"
     ]
    }
   ],
   "source": [
    "root_dir = './'\n",
    "data_dir = './data/'\n",
    "\n",
    "train_data = MedNISTDataset(root_dir=data_dir, section=\"training\", download=True, progress=False, seed=0)\n",
    "train_datalist = []\n",
    "for item in train_data.data:\n",
    "    if item[\"class_name\"] in [\"Hand\", \"HeadCT\"]:\n",
    "        train_datalist.append({\"image\": item[\"image\"], \"class\": 1 if item[\"class_name\"] == \"Hand\" else 2})\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        transforms.RandAffined(\n",
    "            keys=[\"image\"],\n",
    "            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
    "            translate_range=[(-1, 1), (-1, 1)],\n",
    "            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n",
    "            spatial_size=[64, 64],\n",
    "            padding_mode=\"zeros\",\n",
    "            prob=0.5,\n",
    "        ),\n",
    "        transforms.RandLambdad(keys=[\"class\"], prob=0.15, func=lambda x: -1 * torch.ones_like(x)),\n",
    "        transforms.Lambdad(\n",
    "            keys=[\"class\"], func=lambda x: torch.tensor(x, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_ds = CacheDataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "\n",
    "val_data = MedNISTDataset(root_dir=data_dir, section=\"validation\", download=True, progress=False, seed=0)\n",
    "val_datalist = []\n",
    "for item in val_data.data:\n",
    "    if item[\"class_name\"] in [\"Hand\", \"HeadCT\"]:\n",
    "        val_datalist.append({\"image\": item[\"image\"], \"class\": 1 if item[\"class_name\"] == \"Hand\" else 2})\n",
    "\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        transforms.Lambdad(\n",
    "            keys=[\"class\"], func=lambda x: torch.tensor(x, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "val_ds = CacheDataset(data=val_datalist, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763fc3ee-ccc6-4edd-b7db-cec65c3dde4a",
   "metadata": {},
   "source": [
    "### Visualisation of the training images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4f7b9-a66d-4626-ac71-6ec1d5a36873",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = first(train_loader)\n",
    "print(f\"batch shape: {check_data['image'].shape}\")\n",
    "image_visualisation = torch.cat(\n",
    "    [check_data[\"image\"][0, 0], check_data[\"image\"][1, 0], check_data[\"image\"][2, 0], check_data[\"image\"][3, 0]], dim=1\n",
    ")\n",
    "plt.figure(\"training images\", (12, 6))\n",
    "plt.imshow(image_visualisation, vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96ffaa-c768-472f-87e2-4ee4ff7568e4",
   "metadata": {},
   "source": [
    "## Define network, scheduler, optimizer, and inferer\n",
    "\n",
    "We are using the original DDPM scheduler containing `1000` timesteps in its Markov chain, and a 2D UNET with attention mechanisms in the 3rd level, each with 1 attention head (`num_head_channels=64`).\n",
    "\n",
    "In order to pass conditioning variables with dimension of 1 (just specifying the modality of the image), we use `with_conditioning=True, cross_attention_dim=1`.\n",
    "\n",
    "We will also use `DDIMScheduler` to enable faster sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1007c39-1fd8-4c3c-bf23-5589c693f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=(64, 64, 64),\n",
    "    attention_levels=(False, False, True),\n",
    "    num_res_blocks=1,\n",
    "    num_head_channels=(0, 0, 64),\n",
    "    with_conditioning=True,\n",
    "    cross_attention_dim=1,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=2.5e-5)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6bbe56-09b9-4d18-a7ca-95f753edf124",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Here, we are training our model for 75 epochs (training time: ~50 minutes).\n",
    "\n",
    "For the purpose of tutorial, we would like to skip the training and use a pre-trained model instead, set `use_pretrained=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0024a723-ad06-4260-bbdb-5dfc0f7babbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained = True\n",
    "\n",
    "if use_pretrained:\n",
    "    model = DiffusionModelUNet(\n",
    "        spatial_dims=2,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        num_channels=(64, 64, 64),\n",
    "        attention_levels=(False, False, True),\n",
    "        num_res_blocks=1,\n",
    "        num_head_channels=(0, 0, 64),\n",
    "        with_conditioning=True,\n",
    "        cross_attention_dim=1,\n",
    "    )\n",
    "    model.load_state_dict(torch.load('./pretrained/classifier_free.pt'))\n",
    "    model.to(device)\n",
    "else:\n",
    "    n_epochs = 75\n",
    "    val_interval = 5\n",
    "    epoch_loss_list = []\n",
    "    val_epoch_loss_list = []\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    total_start = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        for step, batch in progress_bar:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            classes = batch[\"class\"].to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "            with autocast(enabled=True):\n",
    "                # Generate random noise\n",
    "                noise = torch.randn_like(images).to(device)\n",
    "                # Create timesteps\n",
    "                timesteps = torch.randint(\n",
    "                    0, \n",
    "                    inferer.scheduler.num_train_timesteps, \n",
    "                    (images.shape[0],), \n",
    "                    device=images.device\n",
    "                ).long()\n",
    "                # Get model prediction\n",
    "                noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, condition=classes, timesteps=timesteps)\n",
    "    \n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "    \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "            progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "        epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "    \n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            val_epoch_loss = 0\n",
    "            for step, batch in enumerate(val_loader):\n",
    "                images = batch[\"image\"].to(device)\n",
    "                classes = batch[\"class\"].to(device)\n",
    "                timesteps = torch.randint(\n",
    "                            0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                        ).long()\n",
    "                with torch.no_grad():\n",
    "                    with autocast(enabled=True):\n",
    "                        noise = torch.randn_like(images).to(device)\n",
    "                        noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, condition=classes, timesteps=timesteps)\n",
    "                        val_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "    \n",
    "                val_epoch_loss += val_loss.item()\n",
    "                progress_bar.set_postfix({\"val_loss\": val_epoch_loss / (step + 1)})\n",
    "            val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "\n",
    "            # Sampling image during training\n",
    "            noise = torch.randn((1, 1, 64, 64))\n",
    "            noise = noise.to(device)\n",
    "            condition = (2 * torch.ones((1, 1, 1))).to(device)\n",
    "            scheduler.set_timesteps(num_inference_steps=1000)\n",
    "            with autocast(enabled=True):\n",
    "                image = inferer.sample(input_noise=noise, diffusion_model=model, scheduler=scheduler, conditioning=condition)\n",
    "\n",
    "            plt.figure(figsize=(2, 2))\n",
    "            plt.imshow(image[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "            plt.tight_layout()\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "    \n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"train completed, total time: {total_time}.\")\n",
    "    torch.save(model.state_dict(), 'pretrained/classifier_free.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e6adc-f8ba-4c07-940a-9ac15af166f9",
   "metadata": {},
   "source": [
    "## Sampling process with classifier-free guidance\n",
    "\n",
    "In order to sample using classifier-free guidance, for each step of the process we need to have 2 elements, one generated conditioned in the desired class (here we want to condition on Hands `=1`) and one using the unconditional class (`=-1`). \n",
    "\n",
    "Instead using directly the predicted class in every step, we use the unconditional plus the direction vector pointing to the condition that we want (`noise_pred_text - noise_pred_uncond`). The effect of the condition is defined by the `guidance_scale` defining the influence of our direction vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa9c68-9c33-4319-93b5-96db440a2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "guidance_scale = 7.0\n",
    "conditioning = torch.cat([-1 * torch.ones(1, 1, 1).float(), torch.ones(1, 1, 1).float()], dim=0).to(device)\n",
    "\n",
    "noise = torch.randn((1, 1, 64, 64))\n",
    "noise = noise.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "progress_bar = tqdm(scheduler.timesteps)\n",
    "for t in progress_bar:\n",
    "    with autocast(enabled=True):\n",
    "        with torch.no_grad():\n",
    "            noise_input = torch.cat([noise] * 2)\n",
    "            model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "            noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.imshow(noise[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b7496-5333-45b9-889a-b16937ad19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "guidance_scale = 7.0\n",
    "conditioning = torch.cat([-1 * torch.ones(1, 1, 1).float(), 2*torch.ones(1, 1, 1).float()], dim=0).to(device)\n",
    "\n",
    "noise = torch.randn((1, 1, 64, 64))\n",
    "noise = noise.to(device)\n",
    "\n",
    "scheduler_ddim = DDIMScheduler(\n",
    "    num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195, clip_sample=False\n",
    ")\n",
    "scheduler_ddim.set_timesteps(num_inference_steps=250)\n",
    "progress_bar = tqdm(scheduler_ddim.timesteps)\n",
    "for t in progress_bar:\n",
    "    with autocast(enabled=True):\n",
    "        with torch.no_grad():\n",
    "            noise_input = torch.cat([noise] * 2)\n",
    "            model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "            noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    noise, _ = scheduler_ddim.step(noise_pred, t, noise)\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.imshow(noise[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
